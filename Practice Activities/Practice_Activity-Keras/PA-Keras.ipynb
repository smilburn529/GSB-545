{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-192>:12: DtypeWarning: Columns (8,19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "wine = pd.read_csv(\"cleansingWine.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Variables to predict Price\n",
    "features = ['varieties1', 'abv', 'degree', 'sweet', 'acidity', 'body', 'tannin', 'year']\n",
    "target = 'price'\n",
    "\n",
    "wine_data_clean = wine.dropna(subset=[target])\n",
    "\n",
    "# Convert categorical features into numeric representations using LabelEncoder\n",
    "label_encoders = {}\n",
    "for col in features:\n",
    "    if wine_data_clean[col].dtype == 'object':  \n",
    "        le = LabelEncoder()\n",
    "        wine_data_clean[col] = le.fit_transform(wine_data_clean[col].fillna('Unknown'))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "wine_data_clean = wine_data_clean.dropna(subset=features)\n",
    "\n",
    "X = wine_data_clean[features]\n",
    "y = wine_data_clean[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\u001b[1m  1/136\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 46ms/step\r\u001b[1m  1/136\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 46ms/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m101/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 501us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m101/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 501us/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step\n",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 770us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 - MAE: 99517.93, RMSE: 249835.02, R²: 0.0820\n",
      "Model 1 - MAE: 99517.93, RMSE: 249835.02, R²: 0.0820\n"
     ]
    }
   ],
   "source": [
    "# Build the first model\n",
    "inputs = keras.Input(shape=(X_train_scaled.shape[1],))\n",
    "x = layers.Dense(64, activation='relu')(inputs)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "x = layers.Dense(16, activation='relu')(x)\n",
    "outputs = layers.Dense(1)(x)\n",
    "\n",
    "model1 = keras.Model(inputs=inputs, outputs=outputs, name=\"model_1\")\n",
    "model1.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history1 = model1.fit(X_train_scaled, y_train, validation_split=0.1, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "y_pred1 = model1.predict(X_test_scaled).flatten()\n",
    "\n",
    "mae_1 = mean_absolute_error(y_test, y_pred1)\n",
    "rmse_1 = np.sqrt(mean_squared_error(y_test, y_pred1))\n",
    "r2_1 = r2_score(y_test, y_pred1)\n",
    "\n",
    "print(f\"Model 1 - MAE: {mae_1:.2f}, RMSE: {rmse_1:.2f}, R²: {r2_1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\u001b[1m  1/136\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 39ms/step\r\u001b[1m  1/136\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 39ms/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m125/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 405us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m125/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 405us/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step\n",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 693us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 - MAE: 98499.49, RMSE: 249641.74, R²: 0.0834\n",
      "Model 2 - MAE: 98499.49, RMSE: 249641.74, R²: 0.0834\n"
     ]
    }
   ],
   "source": [
    "# Build the second model\n",
    "inputs = keras.Input(shape=(X_train_scaled.shape[1],))\n",
    "x = layers.Dense(128, activation='relu')(inputs)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "outputs = layers.Dense(1)(x)\n",
    "\n",
    "model2 = keras.Model(inputs=inputs, outputs=outputs, name=\"model_2\")\n",
    "model2.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history2 = model2.fit(X_train_scaled, y_train, validation_split=0.1, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "y_pred2 = model2.predict(X_test_scaled).flatten()\n",
    "\n",
    "mae_2 = mean_absolute_error(y_test, y_pred2)\n",
    "rmse_2 = np.sqrt(mean_squared_error(y_test, y_pred2))\n",
    "r2_2 = r2_score(y_test, y_pred2)\n",
    "\n",
    "print(f\"Model 2 - MAE: {mae_2:.2f}, RMSE: {rmse_2:.2f}, R²: {r2_2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\u001b[1m  1/136\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 32ms/step\r\u001b[1m  1/136\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 32ms/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 387us/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 446us/step\n",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 446us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3 - MAE: 86001.12, RMSE: 259091.83, R²: 0.0127\n",
      "Model 3 - MAE: 86001.12, RMSE: 259091.83, R²: 0.0127\n"
     ]
    }
   ],
   "source": [
    "# Build the third model\n",
    "inputs = keras.Input(shape=(X_train_scaled.shape[1],))\n",
    "x = layers.Dense(64, activation='relu')(inputs)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "x = layers.Dense(16, activation='relu')(x)\n",
    "outputs = layers.Dense(1)(x)\n",
    "\n",
    "model3 = keras.Model(inputs=inputs, outputs=outputs, name=\"model_3\")\n",
    "model3.compile(optimizer='adam', loss='log_cosh', metrics=['mae'])\n",
    "\n",
    "history3 = model3.fit(X_train_scaled, y_train, validation_split=0.1, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "y_pred3 = model3.predict(X_test_scaled).flatten()\n",
    "\n",
    "mae_3 = mean_absolute_error(y_test, y_pred3)\n",
    "rmse_3 = np.sqrt(mean_squared_error(y_test, y_pred3))\n",
    "r2_3 = r2_score(y_test, y_pred3)\n",
    "\n",
    "print(f\"Model 3 - MAE: {mae_3:.2f}, RMSE: {rmse_3:.2f}, R²: {r2_3:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second model preformed the best out of these three. It uses a neural network architecture with three hidden layers (128, 64, and 32 neurons) and dropout regularization to prevent overfitting. It's trained using the Adam optimizer to minimize mean squared error over 50 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Nation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<positron-console-cell-188>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<positron-console-cell-188>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<positron-console-cell-188>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<positron-console-cell-188>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<positron-console-cell-188>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<positron-console-cell-188>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<positron-console-cell-188>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<positron-console-cell-188>:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "<positron-console-cell-188>:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# Change Variables to Predict Nation\n",
    "features = ['varieties1', 'abv', 'degree', 'sweet', 'acidity', 'body', 'tannin', 'year', 'price']\n",
    "target = 'nation'\n",
    "\n",
    "wine_data_clean = wine.dropna(subset=[target])\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "for col in features:\n",
    "    if wine_data_clean[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        wine_data_clean[col] = le.fit_transform(wine_data_clean[col].fillna('Unknown'))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "target_encoder = LabelEncoder()\n",
    "wine_data_clean[target] = target_encoder.fit_transform(wine_data_clean[target])\n",
    "wine_data_clean.dropna(subset=features, inplace=True)\n",
    "\n",
    "# Split into features and labels\n",
    "X = wine_data_clean[features]\n",
    "y = wine_data_clean[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\u001b[1m  1/136\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 27ms/step\r\u001b[1m  1/136\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 27ms/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406us/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471us/step\n",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 1 - Accuracy: 0.5159, Precision: 0.4944, Recall: 0.5159\n",
      "Set 1 - Accuracy: 0.5159, Precision: 0.4944, Recall: 0.5159\n"
     ]
    }
   ],
   "source": [
    "# Build the first model\n",
    "inputs = keras.Input(shape=(X_train_scaled.shape[1],))\n",
    "x = layers.Dense(64, activation='relu')(inputs)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "x = layers.Dense(16, activation='relu')(x)\n",
    "outputs = layers.Dense(len(np.unique(y)), activation='softmax')(x)  # Multiclass output\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"wine_nation_classifier\")\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=keras.optimizers.Adam(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_scaled, y_train, validation_split=0.1, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_probs = model.predict(X_test_scaled)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Evaluate performance\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"Set 1 - Accuracy: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\u001b[1m  1/136\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 30ms/step\r\u001b[1m  1/136\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 30ms/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373us/step\n",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 373us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 2 - Accuracy: 0.4592, Precision: 0.4115, Recall: 0.4592\n",
      "Set 2 - Accuracy: 0.4592, Precision: 0.4115, Recall: 0.4592\n"
     ]
    }
   ],
   "source": [
    "# Build the second model\n",
    "inputs = keras.Input(shape=(X_train_scaled.shape[1],))\n",
    "x = layers.Dense(32, activation='relu')(inputs)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "x = layers.Dense(16, activation='relu')(x)\n",
    "outputs = layers.Dense(len(np.unique(y)), activation='softmax')(x)  # Multiclass output\n",
    "\n",
    "model2 = keras.Model(inputs=inputs, outputs=outputs, name=\"wine_nation_classifier_2\")\n",
    "\n",
    "model2.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history2 = model2.fit(X_train_scaled, y_train, validation_split=0.1, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "y_pred_probs2 = model2.predict(X_test_scaled)\n",
    "y_pred2 = np.argmax(y_pred_probs2, axis=1)\n",
    "\n",
    "acc2 = accuracy_score(y_test, y_pred2)\n",
    "precision2 = precision_score(y_test, y_pred2, average='weighted', zero_division=0)\n",
    "recall2 = recall_score(y_test, y_pred2, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"Set 2 - Accuracy: {acc2:.4f}, Precision: {precision2:.4f}, Recall: {recall2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\u001b[1m  1/136\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 46ms/step\r\u001b[1m  1/136\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 46ms/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 572us/step"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step\n",
      "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m136/136\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 641us/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 3 - Accuracy: 0.5707, Precision: 0.5644, Recall: 0.5707\n",
      "Set 3 - Accuracy: 0.5707, Precision: 0.5644, Recall: 0.5707\n"
     ]
    }
   ],
   "source": [
    "# Build the third model\n",
    "inputs = keras.Input(shape=(X_train_scaled.shape[1],))\n",
    "x = layers.Dense(128, activation='relu')(inputs)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(32, activation='relu')(x)\n",
    "outputs = layers.Dense(len(np.unique(y)), activation='softmax')(x)  # Multiclass output\n",
    "\n",
    "model3 = keras.Model(inputs=inputs, outputs=outputs, name=\"wine_nation_classifier_3\")\n",
    "\n",
    "model3.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.0005),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "history3 = model3.fit(X_train_scaled, y_train, validation_split=0.1, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "y_pred_probs3 = model3.predict(X_test_scaled)\n",
    "y_pred3 = np.argmax(y_pred_probs3, axis=1)\n",
    "\n",
    "acc3 = accuracy_score(y_test, y_pred3)\n",
    "precision3 = precision_score(y_test, y_pred3, average='weighted', zero_division=0)\n",
    "recall3 = recall_score(y_test, y_pred3, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"Set 3 - Accuracy: {acc3:.4f}, Precision: {precision3:.4f}, Recall: {recall3:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third model performs the best. This model uses a deeper architecture with batch normalization layers and a lower learning rat. Its improved accuracy, precision, and recall suggest that the additional layers and normalization helped the network better learn patterns for predicting the nation a wine label comes from."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
